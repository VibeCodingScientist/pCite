# pCite — Build Specification

> *A validation-weighted citation framework that uses familiar researcher incentives
> as the adoption mechanism for a measurement-first scientific knowledge graph.*

---

## Strategy

### Why Citation Metrics Are The Trojan Horse, Not The Product

Scientific publishing is about to be overwhelmed by AI-generated content. Systematic
reviews, computational papers, literature syntheses, observational analyses — all fully
automatable by end of 2025. The citation graph will become infinite, AI-generated, and
gameable at machine speed.

Traditional scientometrics companies (Clarivate, Elsevier) have not internalised this.
They are optimising the map while the territory disappears.

The only epistemically stable substrate in a post-AI world is **physical measurement**.
A mass spectrometer reading from a real biological sample cannot be generated by an LLM.
It is bounded by lab capacity, real time, and the laws of physics. It is the only genuinely
scarce epistemic resource that survives AI at scale.

The correct abstraction hierarchy:

```
WRONG (paper-first, current world):
  Papers → Claims → Measurements (optional provenance)

RIGHT (measurement-first, AI future):
  Measurements → Claims → Papers (discovery interface)
```

But researchers will not adopt a measurement registry. They will adopt a better h-index.

### The Three Phases

**Phase 1 — Trojan Horse**
pCite presents as a smarter citation metric. Researchers see pCite scores and nH-index.
Journals see nIF. Institutions see nImpact. Every familiar incentive is preserved and
enhanced. Under the hood, every claim traces to physical measurements. Nobody needs to
know this yet. Adoption is driven entirely by self-interest.

**Phase 2 — Visible Value**
The measurement layer becomes the differentiator. "This claim has 47 independent instrument
readings across 12 labs" is self-evidently more trustworthy than "this paper has 300 citations."
The vocabulary shifts. Researchers start asking for measurement counts, not citation counts.

**Phase 3 — Radical**
Citations are a legacy compatibility layer. The real graph is measurement → claim → inference
→ knowledge. Papers are a discovery UI sitting on top. AI generates infinite papers — it no
longer matters because papers are not the epistemic substrate. The measurement registry is.

This document describes the PoC that proves Phase 1 works and builds the Phase 3 foundation
invisibly underneath it.

---

## Three Axioms

Every architectural decision derives from these. They are non-negotiable.

**Axiom 1: Claims are universal, papers are evidence containers.**
The assertion *"glutamine increases in IBD patients"* is one knowledge node regardless of
how many papers make it. A claim's identity is determined by its assertion, never by the
paper that made it. Papers are provenance. The same claim in 50 papers is one node with
50 provenance entries. This single decision makes replication a structural property of
the data model rather than something that must be classified.

**Axiom 2: Replication is structural, not classified.**
`len(claim.provenance)` is the replication count. No classifier needed. No NLP needed.
A claim independently made by 8 different labs has `replication_count = 8` automatically.
Log₂ weighting then rewards replication non-linearly — the 8th confirmation adds less than
the 2nd, as any Bayesian would expect.

**Axiom 3: The CI pipeline is a validator.**
Statistically impossible claims cannot be constructed. A p-value of 1.5, a confidence
interval where the lower bound exceeds the upper, a sample size of zero — all raise at
object creation time. Pydantic is the CI pipeline. No separate validation step. No
post-hoc checking. Invalid claims do not exist.

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│  PHASE 3 (measurement registry — built invisibly in Phase 1)   │
│                                                                  │
│  MetaboLights / MassIVE raw instrument files                    │
│         ↓  accession-linked, hash-verified                      │
│  Physical Measurement nodes                                     │
└──────────────────────┬──────────────────────────────────────────┘
                       │ provenance anchor
┌──────────────────────▼──────────────────────────────────────────┐
│  PHASE 1 (claim graph — what researchers interact with)         │
│                                                                  │
│  corpus → extract → validate → graph → evaluate                 │
└─────────────────────────────────────────────────────────────────┘
```

Data flow — everything is files, no services, no databases:

```
PubMed + MetaboLights APIs
        ↓
corpus.py       →   data/papers.jsonl
        ↓
extract.py      →   data/claims.jsonl
        ↓
validate.py     →   data/claims.jsonl  (updated)
                →   data/nanopubs/*.trig
        ↓
graph.py        →   data/graph.graphml
                →   data/scores.jsonl
        ↓
evaluate.py     →   data/results.json
                →   figures/*.pdf
```

A new contributor clones the repo, runs `uv sync`, and the whole system runs locally.
No Docker. No cloud account. No service dependencies.

---

## Repository

```
pcite/
├── pyproject.toml
├── README.md
├── run_poc.py                  ← orchestrator + CI test
├── src/pcite/
│   ├── __init__.py
│   ├── models.py               ← shared contract, only cross-module import
│   ├── corpus.py
│   ├── extract.py
│   ├── validate.py
│   ├── graph.py
│   └── evaluate.py
├── tests/
│   └── test_all.py
├── notebooks/
│   ├── 01_corpus.ipynb
│   ├── 02_extract.ipynb
│   ├── 03_validate.ipynb
│   ├── 04_graph.ipynb
│   └── 05_results.ipynb        ← becomes the paper's results section
└── data/                       ← gitignored, generated by the pipeline
```

---

## Stack

```toml
[project]
name = "pcite"
version = "0.1.0"
description = "Validation-weighted citation framework for reproducible biomedical knowledge graphs"
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.11"

dependencies = [
    "anthropic>=0.40",
    "httpx>=0.27",
    "pydantic>=2.0",
    "rdflib>=7.0",
    "networkx>=3.3",
    "scipy>=1.13",
    "matplotlib>=3.9",
    "jupyter>=1.1",
    "pytest>=8.0",
    "pytest-asyncio>=0.24",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/pcite"]

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]

[tool.ruff]
line-length = 100
src = ["src"]
```

Install: `uv sync`
Run: `uv run python run_poc.py`
Test: `uv run pytest`  ← no API keys needed

---

## Code Standards

Excellence here is measured by what you can remove, not what you add.

**Line count targets:**

| File | Target | Maximum |
|------|--------|---------|
| models.py | 140 | 180 |
| corpus.py | 100 | 140 |
| extract.py | 120 | 160 |
| validate.py | 130 | 170 |
| graph.py | 150 | 200 |
| evaluate.py | 160 | 210 |
| run_poc.py | 50 | 70 |

**Rules:**
1. Every function has a docstring explaining the *why*, not the *what*
2. No function exceeds 30 lines
3. Type hints on every signature
4. No global mutable state
5. Comments explain decisions, never operations
6. `from pcite.models import ...` is the only cross-module import

---

## Milestone 0 — Contract

**`models.py` is finalised and `pytest tests/` is green before any other module is written.**

This is not a suggestion. If the contract is wrong, everything built on top of it is wrong.
An hour correcting models.py before writing modules saves a week of refactoring after.

```python
"""
pcite.models — the shared contract

Three axioms encoded as design decisions:

  1. Claims are universal. ID = hash(subject, predicate, object) — never DOI.
     Same assertion in 50 papers = one node with 50 provenance entries.

  2. Replication is structural. len(claim.provenance) IS the replication count.
     No classifier. No edge type. Structural fact.

  3. CI pipeline is a validator. Statistically impossible claims cannot be
     constructed. Pydantic raises at creation, not at runtime.
"""

from __future__ import annotations
import hashlib, math
from datetime import datetime, timezone
from enum import Enum
from typing import Optional
from pydantic import BaseModel, computed_field, model_validator


class ValidationClass(str, Enum):
    PHYSICAL     = "PhysicalMeasurement"   # raw instrument data in public repo
    CLINICAL     = "ClinicalObservation"   # EHR / IRB-verified patient data
    REPLICATED   = "Replicated"            # ≥3 independent CURATED sources
    CURATED      = "HumanCurated"          # structured deposit, expert record
    AI_GENERATED = "AIGenerated"           # synthesised from literature text
    HYPOTHESIS   = "Hypothesis"            # proposed, untested

VALIDATION_WEIGHT: dict[ValidationClass, float] = {
    ValidationClass.PHYSICAL:     10.0,  # categorically different — instrument data
    ValidationClass.CLINICAL:      4.0,  # physical patient outcomes
    ValidationClass.REPLICATED:    2.0,  # only meaningful if physically anchored
    ValidationClass.CURATED:       0.5,  # human agreement on text
    ValidationClass.AI_GENERATED:  0.01, # near-zero in AI-flood world
    ValidationClass.HYPOTHESIS:    0.0,
}


class Predicate(str, Enum):
    """
    Closed vocabulary. Every metabolomics claim maps to exactly one predicate.
    Closed = queryable and comparable across papers.
    Free strings fragment the graph — "increases", "is elevated in", "upregulates"
    would be three different predicates for the same relationship.
    """
    INCREASES        = "increases"
    DECREASES        = "decreases"
    IS_BIOMARKER_FOR = "is_biomarker_for"
    DISTINGUISHES    = "distinguishes"
    PREDICTS         = "predicts"
    INHIBITS         = "inhibits"
    ACTIVATES        = "activates"
    IS_METABOLITE_OF = "is_metabolite_of"
    CORRELATES_WITH  = "correlates_with"
    CAUSES           = "causes"
    TREATS           = "treats"


class PCiteType(str, Enum):
    SUPPORTS    = "supports"
    EXTENDS     = "extends"
    REPLICATES  = "replicates"   # explicit replication relationship — highest weight
    CONTRADICTS = "contradicts"
    APPLIES     = "applies"

PCITE_WEIGHT: dict[PCiteType, float] = {
    PCiteType.SUPPORTS:    1.0,
    PCiteType.EXTENDS:     1.2,
    PCiteType.REPLICATES:  1.5,  # replication earns more than novelty
    PCiteType.CONTRADICTS: 0.8,  # counter-evidence still earns credit
    PCiteType.APPLIES:     0.6,
}


class Entity(BaseModel):
    id:   str   # "HMDB:HMDB0000122" | "CHEBI:15422" | "MESH:D003920"
    name: str
    type: str   # compound | disease | gene | organism | process | pathway

    @property
    def uri(self) -> str:
        return f"https://identifiers.org/{self.id}"


class StatisticalQualifiers(BaseModel):
    """
    Typed qualifiers that double as a CI pipeline.
    Inconsistent statistics raise at construction — not at runtime, not later.
    This is the entire validation pipeline in one model.
    """
    n:                   Optional[int]                 = None
    p_value:             Optional[float]               = None
    effect_size:         Optional[float]               = None
    confidence_interval: Optional[tuple[float, float]] = None
    fold_change:         Optional[float]               = None
    method:              Optional[str]                 = None

    @model_validator(mode="after")
    def check_consistency(self) -> "StatisticalQualifiers":
        if self.p_value is not None and not (0.0 <= self.p_value <= 1.0):
            raise ValueError(f"p={self.p_value} outside [0, 1]")
        if self.confidence_interval is not None:
            lo, hi = self.confidence_interval
            if lo >= hi:
                raise ValueError(f"CI [{lo}, {hi}]: lower ≥ upper")
        if self.n is not None and self.n < 1:
            raise ValueError(f"n={self.n}: must be ≥ 1")
        return self


class ProvenanceEntry(BaseModel):
    """
    Per-paper evidence record. One claim carries one entry per paper that made it.
    metabo_id is Phase 3's anchor: it links to actual raw instrument files.
    It is stored now, used for classification now, and becomes the measurement
    registry foundation later — without changing a line of code.
    """
    doi:              str
    validation_class: ValidationClass = ValidationClass.AI_GENERATED
    metabo_id:        Optional[str]   = None


class Claim(BaseModel):
    subject:    Entity
    predicate:  Predicate
    object:     Entity
    qualifiers: StatisticalQualifiers
    provenance: list[ProvenanceEntry] = []

    @computed_field
    @property
    def id(self) -> str:
        """
        Content-addressed from assertion alone — never from DOI.
        Same claim in 50 papers = one node with 50 provenance entries.
        This makes replication structural rather than classified.
        """
        payload = f"{self.subject.id}:{self.predicate.value}:{self.object.id}"
        return hashlib.sha256(payload.encode()).hexdigest()[:16]

    @computed_field
    @property
    def replication_count(self) -> int:
        """Number of independent papers making this assertion. Not a classification. A count."""
        return len(self.provenance)

    @computed_field
    @property
    def validation_class(self) -> ValidationClass:
        """Best validation class across provenance entries. Ratchets upward on merge."""
        if not self.provenance:
            return ValidationClass.HYPOTHESIS
        return max(
            self.provenance,
            key=lambda p: VALIDATION_WEIGHT[p.validation_class]
        ).validation_class

    @computed_field
    @property
    def base_weight(self) -> float:
        """
        Evidence quality × replication count, log-scaled.

        The 1000× gap between PHYSICAL (10.0) and AI_GENERATED (0.01)
        is the paper's central claim made quantitative: a claim anchored
        to instrument data is categorically, not incrementally, more
        trustworthy than a claim that exists only as text.

        PHYSICAL, 8 papers:     10.0 × log₂(9) ≈ 31.7
        AI_GENERATED, 1 paper:  0.01 × log₂(2) =  0.01

        3170× difference. In a world of infinite AI-generated text,
        this asymmetry is not a design choice — it is an epistemological fact.
        """
        return VALIDATION_WEIGHT[self.validation_class] * math.log2(self.replication_count + 1)

    def merge(self, other: "Claim") -> "Claim":
        """
        Same assertion, different paper. Accumulate provenance.
        Best-wins per DOI: if a DOI appears in both, keep the higher-trust entry.
        Validation class updates automatically via computed field — no bookkeeping.
        """
        assert self.id == other.id
        by_doi = {p.doi: p for p in self.provenance}
        for p in other.provenance:
            existing = by_doi.get(p.doi)
            if not existing or (
                VALIDATION_WEIGHT[p.validation_class] > VALIDATION_WEIGHT[existing.validation_class]
            ):
                by_doi[p.doi] = p
        return self.model_copy(update={"provenance": list(by_doi.values())})


class PCite(BaseModel):
    source_id:     str
    target_id:     str
    type:          PCiteType
    source_weight: float

    @computed_field
    @property
    def weight(self) -> float:
        """
        Edge weight = citation type × source trustworthiness.

        Replication from a physically-validated, 8× replicated claim:  1.5 × 31.7 ≈ 47.6
        Support from an AI-generated singleton:                         1.0 × 0.01  =  0.01

        4760× difference. No manual scoring. Entirely from the data model.
        """
        return PCITE_WEIGHT[self.type] * self.source_weight


class Paper(BaseModel):
    doi:                   str
    pmid:                  Optional[str] = None
    title:                 str
    abstract:              str
    full_text:             Optional[str] = None
    metabo_id:             Optional[str] = None
    traditional_citations: int           = 0
    year:                  Optional[int] = None
```

---

## Milestone 1 — Corpus

Fetch the metabolomics paper corpus from PubMed. For each paper, check whether raw
instrument data exists in MetaboLights or MassIVE. That single check — physical deposit
exists? — is the foundation of the validation system and Phase 3's first foothold.

```python
"""
pcite.corpus

Fetch metabolomics papers from PubMed.
Check each for MetaboLights / MassIVE raw data deposits.
The deposit check is the Phase 3 measurement anchor embedded in Phase 1.

Output: data/papers.jsonl
Run:    python -m pcite.corpus
"""

import asyncio, json, sys, urllib.parse
from pathlib import Path
import httpx
from pcite.models import Paper

PUBMED_SEARCH = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
PUBMED_FETCH  = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
METABO_DOI    = "https://www.ebi.ac.uk/metabolights/ws/studies/doi"
DATA_FILE     = Path("data/papers.jsonl")
DEFAULT_QUERY = '("metabolomics"[MeSH] OR "metabolomics"[Title]) AND "biomarker"[Title/Abstract]'

_sem = asyncio.Semaphore(3)   # PubMed: 3 req/s without API key


async def _search_pmids(query: str, max_results: int, client: httpx.AsyncClient) -> list[str]:
    resp = await client.get(PUBMED_SEARCH, params={
        "db": "pubmed", "term": query, "retmax": max_results,
        "retmode": "json", "datetype": "pdat", "mindate": "2015",
    })
    return resp.json()["esearchresult"]["idlist"]


async def _check_deposit(doi: str, client: httpx.AsyncClient) -> str | None:
    """
    Non-null return = physical instrument data exists in MetaboLights.
    This is the only field that determines ValidationClass.PHYSICAL eligibility.
    """
    async with _sem:
        try:
            resp = await client.get(
                f"{METABO_DOI}/{urllib.parse.quote(doi)}", timeout=10
            )
            if resp.status_code == 200 and (data := resp.json()):
                return data[0].get("accession")
        except Exception:
            pass
    return None


async def _fetch_paper(pmid: str, client: httpx.AsyncClient) -> Paper | None:
    async with _sem:
        try:
            resp = await client.get(PUBMED_FETCH, params={
                "db": "pubmed", "id": pmid, "retmode": "xml", "rettype": "abstract"
            })
            xml   = resp.text
            doi   = _between(xml, '<ELocationID EIdType="doi"', "</ELocationID>").split(">")[-1]
            title = _between(xml, "<ArticleTitle>", "</ArticleTitle>")
            abst  = _between(xml, "<AbstractText>", "</AbstractText>")
            year  = _between(xml, "<PubDate><Year>", "</Year>")
            if not doi or not title:
                return None
            return Paper(
                doi=doi.strip(), pmid=pmid, title=title, abstract=abst,
                metabo_id=await _check_deposit(doi.strip(), client),
                year=int(year) if year.isdigit() else None,
            )
        except Exception:
            return None


def _between(text: str, start: str, end: str) -> str:
    try:
        s = text.index(start) + len(start)
        return text[s:text.index(end, s)].strip()
    except ValueError:
        return ""


async def build_corpus(query: str = DEFAULT_QUERY, max_results: int = 2000) -> int:
    DATA_FILE.parent.mkdir(exist_ok=True)
    async with httpx.AsyncClient(timeout=30, follow_redirects=True) as client:
        pmids  = await _search_pmids(query, max_results, client)
        papers = await asyncio.gather(*[_fetch_paper(p, client) for p in pmids])
    valid = [p for p in papers if p]
    with DATA_FILE.open("w") as f:
        for p in valid:
            f.write(p.model_dump_json() + "\n")
    deposited = sum(1 for p in valid if p.metabo_id)
    print(f"  {len(valid)} papers, {deposited} with MetaboLights deposits", file=sys.stderr)
    return len(valid)


def load_papers() -> list[Paper]:
    return [Paper.model_validate_json(l) for l in DATA_FILE.read_text().splitlines() if l]


if __name__ == "__main__":
    asyncio.run(build_corpus())
```

**Milestone 1 complete when:** `data/papers.jsonl` exists with ~2,000 entries.
Print the MetaboLights deposit count — this is your Phase 3 seed size.

---

## Milestone 2 — Extraction

Transform paper text into structured `Claim` objects. The entire NLP pipeline is one
well-engineered prompt plus Pydantic validation. The merge step is where replication
becomes a structural fact for the first time.

```python
"""
pcite.extract

Paper text → structured Claims via Claude tool_use.
The merge step collapses identical assertions across papers into one node.
After this: len(claim.provenance) == replication count.

Output: data/claims.jsonl (deduplicated)
Run:    python -m pcite.extract
"""

import asyncio, functools, json, sys, urllib.request
from pathlib import Path
import anthropic
from pcite.models import (
    Claim, Entity, Paper, Predicate, ProvenanceEntry,
    StatisticalQualifiers, ValidationClass,
)

DATA_OUT = Path("data/claims.jsonl")
_client  = anthropic.AsyncAnthropic()
_sem     = asyncio.Semaphore(5)

_PROMPT = """\
Extract atomic scientific claims from this metabolomics paper.

A claim is one falsifiable assertion: subject → predicate → object, with quantitative evidence.
Only extract claims backed by numbers (p-values, fold changes, effect sizes, AUC).

predicate MUST be exactly one of:
  increases | decreases | is_biomarker_for | distinguishes | predicts |
  inhibits | activates | is_metabolite_of | correlates_with | causes | treats

Entity IDs: HMDB (compounds), MESH (diseases), CHEBI (chemicals).
Format: "HMDB:HMDB0000122" or "MESH:D006262". Use "UNKNOWN:name" if unmappable.

DOI: {doi}

{text}"""

_SCHEMA = {
    "name": "submit_claims",
    "description": "Submit extracted scientific claims",
    "input_schema": {
        "type": "object",
        "required": ["claims"],
        "properties": {"claims": {"type": "array", "items": {
            "type": "object",
            "required": ["subject_name","subject_type","subject_id",
                          "predicate","object_name","object_type","object_id"],
            "properties": {
                "subject_name": {"type": "string"},
                "subject_type": {"type": "string"},
                "subject_id":   {"type": "string"},
                "predicate":    {"enum": [p.value for p in Predicate]},
                "object_name":  {"type": "string"},
                "object_type":  {"type": "string"},
                "object_id":    {"type": "string"},
                "n":            {"type": ["integer","null"]},
                "p_value":      {"type": ["number","null"]},
                "effect_size":  {"type": ["number","null"]},
                "fold_change":  {"type": ["number","null"]},
                "method":       {"type": ["string","null"]},
            }
        }}}
    }
}


@functools.lru_cache(maxsize=10_000)
def _hmdb_lookup(name: str) -> str:
    """HMDB compound lookup. Cached — same compound appears in hundreds of papers."""
    try:
        url = f"https://hmdb.ca/metabolites/search.json?query={urllib.parse.quote(name)}"
        with urllib.request.urlopen(url, timeout=5) as r:
            if hits := json.loads(r.read()).get("metabolites"):
                return f"HMDB:{hits[0]['accession']}"
    except Exception:
        pass
    return f"UNKNOWN:{name.lower().strip()}"


async def _extract(paper: Paper) -> list[Claim]:
    """One Claude call per paper. tool_use guarantees valid JSON — no output parsing."""
    text = paper.title + "\n\n" + paper.abstract
    if paper.full_text:
        text += "\n\n" + paper.full_text[:3000]
    async with _sem:
        try:
            resp = await _client.messages.create(
                model="claude-opus-4-6",
                max_tokens=2048,
                tools=[_SCHEMA],
                tool_choice={"type": "tool", "name": "submit_claims"},
                messages=[{"role": "user",
                            "content": _PROMPT.format(doi=paper.doi, text=text)}],
            )
            raw = resp.content[0].input["claims"]
        except Exception as e:
            print(f"  skip {paper.doi}: {e}", file=sys.stderr)
            return []

    claims = []
    for c in raw:
        try:
            subj_id = c["subject_id"] if not c["subject_id"].startswith("UNKNOWN") \
                      else _hmdb_lookup(c["subject_name"])
            claims.append(Claim(
                subject    = Entity(id=subj_id, name=c["subject_name"], type=c["subject_type"]),
                predicate  = Predicate(c["predicate"]),
                object     = Entity(id=c["object_id"], name=c["object_name"], type=c["object_type"]),
                qualifiers = StatisticalQualifiers(
                    n=c.get("n"), p_value=c.get("p_value"),
                    effect_size=c.get("effect_size"), fold_change=c.get("fold_change"),
                    method=c.get("method"),
                ),
                provenance = [ProvenanceEntry(
                    doi=paper.doi, metabo_id=paper.metabo_id,
                    validation_class=ValidationClass.AI_GENERATED,
                )],
            ))
        except Exception as e:
            print(f"  bad claim in {paper.doi}: {e}", file=sys.stderr)
    return claims


def _merge_all(claims: list[Claim]) -> list[Claim]:
    """
    Deduplicate by claim ID, merging provenance.
    After this call, len(claim.provenance) is the true replication count.
    This is the moment replication becomes a structural property.
    """
    merged: dict[str, Claim] = {}
    for c in claims:
        merged[c.id] = merged[c.id].merge(c) if c.id in merged else c
    return list(merged.values())


async def process_corpus() -> int:
    from pcite.corpus import load_papers
    papers, all_claims = load_papers(), []
    for i in range(0, len(papers), 20):
        results = await asyncio.gather(*[_extract(p) for p in papers[i:i+20]])
        for claims in results:
            all_claims.extend(claims)
        print(f"  {len(all_claims)} claims extracted...", file=sys.stderr)
    merged = _merge_all(all_claims)
    DATA_OUT.parent.mkdir(exist_ok=True)
    with DATA_OUT.open("w") as f:
        for c in merged:
            f.write(c.model_dump_json() + "\n")
    replicated = sum(1 for c in merged if c.replication_count > 1)
    print(f"  {len(merged)} unique claims, {replicated} seen in >1 paper", file=sys.stderr)
    return len(merged)


def load_claims() -> list[Claim]:
    return [Claim.model_validate_json(l) for l in DATA_OUT.read_text().splitlines() if l]


if __name__ == "__main__":
    asyncio.run(process_corpus())
```

**Milestone 2 complete when:** `data/claims.jsonl` exists. Manually review 50 random
claims for extraction quality before proceeding. If precision is below 70%, revise the
prompt — the prompt is the product of this module, the code is plumbing.

---

## Milestone 3 — Validation & Nanopublications

Upgrade each claim's ValidationClass based on checkable external facts. Serialise every
claim as a W3C nanopublication — the permanent, citable scientific objects that give this
work credibility in the semantic publishing community.

```python
"""
pcite.validate

Two jobs:
  1. Upgrade ProvenanceEntry.validation_class based on checkable facts.
  2. Serialise each claim as a W3C TriG nanopublication.

The classifier is a pure function — testable with no mocks, no network.
Every rule has a comment explaining the epistemological reason.

Output: data/claims.jsonl (updated) + data/nanopubs/*.trig
Run:    python -m pcite.validate
"""

from __future__ import annotations
from datetime import datetime, timezone
from pathlib import Path
import sys, rdflib
from rdflib import RDF, XSD, Literal, Namespace, URIRef
from rdflib.graph import ConjunctiveGraph
from pcite.models import Claim, Paper, ProvenanceEntry, ValidationClass, VALIDATION_WEIGHT

DATA_CLAIMS = Path("data/claims.jsonl")
NANOPUBS    = Path("data/nanopubs")

NP   = Namespace("https://w3id.org/np/")
NPX  = Namespace("http://purl.org/nanopub/x/")
PROV = Namespace("http://www.w3.org/ns/prov#")
DCT  = Namespace("http://purl.org/dc/terms/")
BASE = Namespace("https://pcite.org/np/")


def classify_provenance(entry: ProvenanceEntry, papers: dict[str, Paper]) -> ProvenanceEntry:
    """
    Upgrade a ProvenanceEntry's ValidationClass based on checkable external facts.
    First match wins.

    PHYSICAL: paper has a MetaboLights accession.
      Reason: raw instrument files exist in a public repository. The claim is
      anchored to a physical measurement that anyone can download and reanalyse.

    CURATED: paper has a structured abstract record.
      Reason: a human bibliographer verified the record. Weaker than physical
      data but stronger than AI text synthesis.

    AI_GENERATED: no verifiable external anchor.
      Reason: the claim exists only as text. May be accurate but unverifiable.

    Replication is NOT classified here. It emerges from provenance count in upgrade_claim().
    """
    paper = papers.get(entry.doi)
    if not paper:
        return entry
    if paper.metabo_id:
        return entry.model_copy(update={
            "validation_class": ValidationClass.PHYSICAL,
            "metabo_id": paper.metabo_id,
        })
    if paper.abstract:
        return entry.model_copy(update={"validation_class": ValidationClass.CURATED})
    return entry


def upgrade_claim(claim: Claim, papers: dict[str, Paper]) -> Claim:
    """
    Upgrade all entries, then apply structural replication rule:
    ≥3 independent CURATED-or-better sources → REPLICATED.
    """
    upgraded = [classify_provenance(p, papers) for p in claim.provenance]
    trusted  = sum(
        1 for p in upgraded
        if VALIDATION_WEIGHT[p.validation_class] >= VALIDATION_WEIGHT[ValidationClass.CURATED]
    )
    if trusted >= 3:
        upgraded = [
            p.model_copy(update={"validation_class": ValidationClass.REPLICATED})
            if VALIDATION_WEIGHT[p.validation_class] >= VALIDATION_WEIGHT[ValidationClass.CURATED]
            else p
            for p in upgraded
        ]
    return claim.model_copy(update={"provenance": upgraded})


def to_nanopub(claim: Claim) -> ConjunctiveGraph:
    """
    Four named graphs per W3C nanopublication spec.
    Assertion = exactly one RDF triple.
    Provenance links each paper + any MetaboLights deposit (Phase 3 anchor).
    """
    g, base = ConjunctiveGraph(), BASE[claim.id + "/"]
    head    = rdflib.Graph(g.store, identifier=base + "head")
    assn    = rdflib.Graph(g.store, identifier=base + "assertion")
    prov    = rdflib.Graph(g.store, identifier=base + "provenance")
    pubinfo = rdflib.Graph(g.store, identifier=base + "pubinfo")

    head.add((base, RDF.type,             NP.Nanopublication))
    head.add((base, NP.hasAssertion,      base + "assertion"))
    head.add((base, NP.hasProvenance,     base + "provenance"))
    head.add((base, NP.hasPublicationInfo, base + "pubinfo"))

    assn.add((URIRef(claim.subject.uri),
              BASE[f"predicate/{claim.predicate.value}"],
              URIRef(claim.object.uri)))

    assn_uri = base + "assertion"
    for entry in claim.provenance:
        doi_uri = URIRef(f"https://doi.org/{entry.doi}")
        prov.add((assn_uri, PROV.wasDerivedFrom, doi_uri))
        if entry.metabo_id:
            prov.add((doi_uri, PROV.hadPrimarySource,
                      URIRef(f"https://www.ebi.ac.uk/metabolights/{entry.metabo_id}")))
    prov.add((assn_uri, NPX.validationClass,
              Literal(claim.validation_class.value)))
    prov.add((assn_uri, NPX.replicationCount,
              Literal(claim.replication_count, datatype=XSD.integer)))

    pubinfo.add((base, DCT.created,
                 Literal(datetime.now(timezone.utc).isoformat(), datatype=XSD.dateTime)))
    pubinfo.add((base, DCT.publisher, URIRef("https://pcite.org")))
    return g


def process_claims() -> int:
    from pcite.corpus import load_papers
    from pcite.extract import load_claims
    papers, claims = {p.doi: p for p in load_papers()}, load_claims()
    NANOPUBS.mkdir(parents=True, exist_ok=True)
    upgraded = [upgrade_claim(c, papers) for c in claims]
    for c in upgraded:
        to_nanopub(c).serialize(
            destination=str(NANOPUBS / f"{c.id}.trig"), format="trig"
        )
    with DATA_CLAIMS.open("w") as f:
        for c in upgraded:
            f.write(c.model_dump_json() + "\n")
    by_class: dict[str, int] = {}
    for c in upgraded:
        by_class[c.validation_class.value] = by_class.get(c.validation_class.value, 0) + 1
    for vc, n in sorted(by_class.items(), key=lambda x: -x[1]):
        print(f"  {vc:<25} {n:>5}", file=sys.stderr)
    return len(upgraded)


if __name__ == "__main__":
    process_claims()
```

**Milestone 3 complete when:** `data/nanopubs/` contains one `.trig` per claim.
Open one in a text editor. Verify four named graphs. Verify the assertion is a single
RDF triple. Print the ValidationClass distribution — a meaningful fraction should be
PHYSICAL if MetaboLights deposits were found in Milestone 1.

---

## Milestone 4 — Graph & Scoring

Build the directed weighted graph, classify citation edges, compute pCite scores.
The scoring formula is the paper's core scientific claim.

```python
"""
pcite.graph

Build pCite graph. Compute scores.

pCite score = Σ (PCiteType.weight × source.base_weight) for all incoming edges
base_weight = ValidationClass.weight × log₂(replication_count + 1)

The formula is 5 lines. The scientific argument is in the constants.

Output: data/graph.graphml + data/scores.jsonl
Run:    python -m pcite.graph
"""

import asyncio, functools, json, sys
from pathlib import Path
import httpx, networkx as nx, anthropic
from pcite.models import Claim, PCiteType, PCITE_WEIGHT

GRAPH_OUT  = Path("data/graph.graphml")
SCORES_OUT = Path("data/scores.jsonl")
OPENALEX   = "https://api.openalex.org/works"

_client = anthropic.Anthropic()
_sem    = asyncio.Semaphore(5)


def build_claim_nodes(claims: list[Claim]) -> nx.DiGraph:
    G = nx.DiGraph()
    for c in claims:
        G.add_node(c.id, **{
            "validation_class":  c.validation_class.value,
            "base_weight":       c.base_weight,
            "replication_count": c.replication_count,
            "predicate":         c.predicate.value,
            "subject":           c.subject.name,
            "object":            c.object.name,
        })
    return G


async def _citing_dois(doi: str, client: httpx.AsyncClient) -> list[str]:
    """OpenAlex citation lookup. Fully open, no auth, 100k req/day."""
    async with _sem:
        try:
            resp = await client.get(OPENALEX, params={
                "filter": f"cites:{doi}", "select": "doi",
                "per-page": 50, "mailto": "research@pcite.org",
            }, timeout=15)
            return [
                w["doi"].replace("https://doi.org/", "")
                for w in resp.json().get("results", []) if w.get("doi")
            ]
        except Exception:
            return []


@functools.lru_cache(maxsize=50_000)
def _classify(src_text: str, tgt_text: str) -> PCiteType:
    """
    Classify citation relationship. Claude Haiku — fast and cheap for single-word output.
    Cached: most claim pairs recur across the corpus.
    """
    resp = _client.messages.create(
        model="claude-haiku-4-5-20251001",
        max_tokens=10,
        messages=[{"role": "user", "content":
            f'Source: "{src_text}"\nTarget: "{tgt_text}"\n\n'
            f"One word:\nsupports | extends | replicates | contradicts | applies"}]
    )
    word = resp.content[0].text.strip().lower().split()[0]
    return PCiteType(word) if word in PCiteType._value2member_map_ else PCiteType.SUPPORTS


def compute_pcite_scores(G: nx.DiGraph) -> dict[str, float]:
    """
    pCite score = Σ incoming edge weights.

    weight per edge = PCiteType.weight × source.base_weight
    source.base_weight = ValidationClass.weight × log₂(replication_count + 1)

    Reading the graph structure is sufficient to compute trust.
    No manual scoring. No external labels.
    """
    return {
        node: sum(d["weight"] for _, _, d in G.in_edges(node, data=True))
        for node in G.nodes()
    }


async def build_full_graph() -> nx.DiGraph:
    from pcite.extract import load_claims
    claims = load_claims()
    by_doi: dict[str, list[Claim]] = {}
    for c in claims:
        for p in c.provenance:
            by_doi.setdefault(p.doi, []).append(c)

    G = build_claim_nodes(claims)
    print(f"  {G.number_of_nodes()} nodes", file=sys.stderr)

    async with httpx.AsyncClient(follow_redirects=True) as client:
        dois       = list(by_doi.keys())
        citing_map = dict(zip(
            dois,
            await asyncio.gather(*[_citing_dois(doi, client) for doi in dois])
        ))

    edges = 0
    for target_doi, citing_dois_list in citing_map.items():
        for tgt in by_doi.get(target_doi, []):
            for citing_doi in citing_dois_list:
                for src in by_doi.get(citing_doi, []):
                    if src.id == tgt.id:
                        continue
                    ntype  = _classify(
                        f"{src.subject.name} {src.predicate.value} {src.object.name}",
                        f"{tgt.subject.name} {tgt.predicate.value} {tgt.object.name}",
                    )
                    G.add_edge(src.id, tgt.id, type=ntype.value,
                               weight=PCITE_WEIGHT[ntype] * src.base_weight,
                               source_weight=src.base_weight)
                    edges += 1

    print(f"  {edges} edges", file=sys.stderr)
    scores = compute_pcite_scores(G)
    nx.set_node_attributes(G, scores, "pcite_score")
    nx.write_graphml(G, GRAPH_OUT)

    with SCORES_OUT.open("w") as f:
        for c in claims:
            f.write(json.dumps({
                "claim_id":          c.id,
                "pcite_score":       scores.get(c.id, 0.0),
                "validation_class":  c.validation_class.value,
                "replication_count": c.replication_count,
                "base_weight":       c.base_weight,
                "predicate":         c.predicate.value,
                "subject":           c.subject.name,
                "object":            c.object.name,
            }) + "\n")
    return G


if __name__ == "__main__":
    G = asyncio.run(build_full_graph())
    print(f"✓ {G.number_of_nodes()} nodes, {G.number_of_edges()} edges", file=sys.stderr)
```

**Milestone 4 complete when:** `data/scores.jsonl` exists. Inspect the top-10 claims
by pCite score manually. They should be biologically sensible — well-known metabolomics
findings appearing in multiple papers. If not, the extraction or merge is wrong.

---

## Milestone 5 — Evaluation

Three statistical tests measuring whether pCite surfaces physically-validated claims
better than traditional citation counts. All three must agree.

**Ground truth:** `ValidationClass.PHYSICAL` claims — self-selected via MetaboLights
deposits. As objectively validated as metabolomics claims can be.

```python
"""
pcite.evaluate

Three tests, all measuring whether pCite surfaces physically-validated claims
better than traditional citation count. They must agree. If they don't: debug.

Output: data/results.json + figures/*.pdf
Run:    python -m pcite.evaluate
"""

import json, math, sys
from pathlib import Path
from scipy import stats
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

SCORES_IN   = Path("data/scores.jsonl")
RESULTS_OUT = Path("data/results.json")
FIGURES_DIR = Path("figures")

plt.rcParams.update({
    "font.family":       "serif",
    "font.size":         11,
    "axes.spines.top":   False,
    "axes.spines.right": False,
    "figure.dpi":        150,
})

COLORS = {
    "PhysicalMeasurement": "#d62728",
    "Replicated":          "#ff7f0e",
    "HumanCurated":        "#2ca02c",
    "AIGenerated":         "#aec7e8",
    "Hypothesis":          "#c7c7c7",
}


def load_scores() -> list[dict]:
    return [json.loads(l) for l in SCORES_IN.read_text().splitlines() if l]


def mann_whitney(records: list[dict]) -> dict:
    """Non-parametric. No distribution assumption. One-sided."""
    val   = [r["pcite_score"] for r in records if r["validation_class"] == "PhysicalMeasurement"]
    unval = [r["pcite_score"] for r in records if r["validation_class"] != "PhysicalMeasurement"]
    u, p  = stats.mannwhitneyu(val, unval, alternative="greater")
    return {"u": u, "p_value": p, "n_validated": len(val), "n_unvalidated": len(unval),
            "median_validated":   float(sorted(val)[len(val)//2]) if val else 0,
            "median_unvalidated": float(sorted(unval)[len(unval)//2]) if unval else 0}


def precision_at_k(records: list[dict], k: int = 50) -> dict:
    """What fraction of the top-k are physically validated?"""
    validated = {r["claim_id"] for r in records if r["validation_class"] == "PhysicalMeasurement"}
    nc = sorted(records, key=lambda r: r["pcite_score"], reverse=True)
    tr = sorted(records, key=lambda r: r.get("traditional_citations", 0), reverse=True)
    p_nc = sum(1 for r in nc[:k] if r["claim_id"] in validated) / k
    p_tr = sum(1 for r in tr[:k] if r["claim_id"] in validated) / k
    return {"k": k, "precision_pcite": p_nc, "precision_traditional": p_tr,
            "lift": p_nc / p_tr if p_tr > 0 else float("inf")}


def ndcg_at_k(records: list[dict], k: int = 50) -> dict:
    """Normalised Discounted Cumulative Gain. Standard IR metric."""
    validated = {r["claim_id"] for r in records if r["validation_class"] == "PhysicalMeasurement"}

    def dcg(ranked: list[dict]) -> float:
        return sum(
            (1 if r["claim_id"] in validated else 0) / math.log2(i + 2)
            for i, r in enumerate(ranked[:k])
        )

    ideal = sorted(records, key=lambda r: r["validation_class"] == "PhysicalMeasurement",
                   reverse=True)
    idcg  = dcg(ideal)
    if idcg == 0:
        return {"k": k, "ndcg_pcite": 0.0, "ndcg_traditional": 0.0}
    nc = sorted(records, key=lambda r: r["pcite_score"], reverse=True)
    tr = sorted(records, key=lambda r: r.get("traditional_citations", 0), reverse=True)
    return {"k": k, "ndcg_pcite": dcg(nc) / idcg, "ndcg_traditional": dcg(tr) / idcg}


def fig1_rank_scatter(records: list[dict]) -> plt.Figure:
    """
    x = traditional rank, y = pCite rank, colour = validation class.
    Physical claims should cluster top-left. This is the paper's hero figure.
    """
    nc = {r["claim_id"]: i+1 for i, r in enumerate(
        sorted(records, key=lambda r: r["pcite_score"], reverse=True))}
    tr = {r["claim_id"]: i+1 for i, r in enumerate(
        sorted(records, key=lambda r: r.get("traditional_citations",0), reverse=True))}
    fig, ax = plt.subplots(figsize=(7, 6))
    for vc, color in COLORS.items():
        group = [r for r in records if r["validation_class"] == vc]
        ax.scatter([tr[r["claim_id"]] for r in group],
                   [nc[r["claim_id"]] for r in group],
                   c=color, label=vc, alpha=0.5, s=10, linewidths=0)
    n = len(records)
    ax.plot([1, n], [1, n], "k--", lw=0.8, alpha=0.3, label="No change")
    ax.set(xlabel="Traditional citation rank", ylabel="pCite rank",
           title="pCite vs traditional ranking")
    ax.legend(fontsize=8, markerscale=2, loc="lower right")
    for axis in [ax.xaxis, ax.yaxis]:
        axis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f"{int(x):,}"))
    fig.tight_layout()
    return fig


def fig2_score_distribution(records: list[dict]) -> plt.Figure:
    """pCite score by validation class. Physical should be dramatically higher."""
    classes = ["PhysicalMeasurement", "Replicated", "HumanCurated", "AIGenerated"]
    data    = [[r["pcite_score"] for r in records if r["validation_class"] == vc] for vc in classes]
    fig, ax = plt.subplots(figsize=(7, 5))
    bp = ax.boxplot(data, labels=["Physical","Replicated","Curated","AI"],
                    patch_artist=True, showfliers=False)
    for patch, vc in zip(bp["boxes"], classes):
        patch.set(facecolor=COLORS[vc], alpha=0.7)
    ax.set(ylabel="pCite score", title="pCite score by validation class")
    fig.tight_layout()
    return fig


def fig3_precision_curve(records: list[dict]) -> plt.Figure:
    """Precision@k for k = 10…200. pCite should stay above traditional throughout."""
    validated = {r["claim_id"] for r in records if r["validation_class"] == "PhysicalMeasurement"}
    nc = sorted(records, key=lambda r: r["pcite_score"], reverse=True)
    tr = sorted(records, key=lambda r: r.get("traditional_citations", 0), reverse=True)
    ks = list(range(10, min(201, len(records)), 10))
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.plot(ks, [sum(1 for r in nc[:k] if r["claim_id"] in validated)/k for k in ks],
            "-o", ms=4, color=COLORS["PhysicalMeasurement"], label="pCite")
    ax.plot(ks, [sum(1 for r in tr[:k] if r["claim_id"] in validated)/k for k in ks],
            "-s", ms=4, color=COLORS["AIGenerated"], label="Traditional")
    ax.axhline(len(validated)/len(records), ls="--", color="gray", lw=0.8, label="Random")
    ax.set(xlabel="k", ylabel="Precision@k", title="Precision of surfacing validated claims")
    ax.legend()
    fig.tight_layout()
    return fig


def run_experiment() -> dict:
    records = load_scores()
    results = {
        "n_total":      len(records),
        "mann_whitney": mann_whitney(records),
        "precision_50": precision_at_k(records, k=50),
        "ndcg_50":      ndcg_at_k(records, k=50),
    }
    RESULTS_OUT.parent.mkdir(exist_ok=True)
    RESULTS_OUT.write_text(json.dumps(results, indent=2))
    FIGURES_DIR.mkdir(exist_ok=True)
    fig1_rank_scatter(records).savefig(FIGURES_DIR / "fig1_rank_comparison.pdf")
    fig2_score_distribution(records).savefig(FIGURES_DIR / "fig2_score_dist.pdf")
    fig3_precision_curve(records).savefig(FIGURES_DIR / "fig3_precision_at_k.pdf")
    plt.close("all")
    return results


if __name__ == "__main__":
    r   = run_experiment()
    mw  = r["mann_whitney"]
    p50 = r["precision_50"]
    ng  = r["ndcg_50"]
    print(f"\n{'─'*55}", file=sys.stderr)
    print(f"  Mann-Whitney p:  {mw['p_value']:.4f}  "
          f"{'✓' if mw['p_value'] < 0.05 else '✗'}", file=sys.stderr)
    print(f"  Precision@50:    pCite={p50['precision_pcite']:.3f}  "
          f"trad={p50['precision_traditional']:.3f}  "
          f"lift={p50['lift']:.1f}×", file=sys.stderr)
    print(f"  NDCG@50:         pCite={ng['ndcg_pcite']:.4f}  "
          f"trad={ng['ndcg_traditional']:.4f}", file=sys.stderr)
    print(f"{'─'*55}", file=sys.stderr)
```

**Milestone 5 complete when:** All three tests show pCite outperforming traditional
citations and agree with each other. If they disagree, there is a data bug. If pCite
loses on all three, revisit whether MetaboLights deposits are correctly propagating
through to edge weights.

---

## Orchestrator

```python
"""
run_poc.py

Exits 0 if hypothesis holds. Exits 1 if it does not.
This file is simultaneously the demo, the integration test,
and the reproducibility proof.

Usage:
  uv run python run_poc.py
  uv run python run_poc.py --dry-run   # evaluate cached data, skip API calls
"""

import asyncio, sys, argparse
from pcite import corpus, extract, validate, graph, evaluate


async def main(dry_run: bool = False) -> int:
    print("\npCite PoC — Validation-Weighted Citation Graph for Metabolomics\n")

    if not dry_run:
        print("1/5  Corpus (PubMed + MetaboLights)...")
        print(f"     {await corpus.build_corpus()} papers\n")

        print("2/5  Extraction (Claude Opus)...")
        print(f"     {await extract.process_corpus()} unique claims\n")

        print("3/5  Validation + nanopublications...")
        print(f"     {validate.process_claims()} claims classified\n")

        print("4/5  pCite graph (OpenAlex + Claude Haiku)...")
        G = await graph.build_full_graph()
        print(f"     {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\n")

    print("5/5  Experiment...")
    r   = evaluate.run_experiment()
    mw  = r["mann_whitney"]
    p50 = r["precision_50"]
    ng  = r["ndcg_50"]

    print(f"\n{'━'*55}")
    print(f"  n total:          {r['n_total']:,}")
    print(f"  n validated:      {mw['n_validated']:,}")
    print(f"  Mann-Whitney p:   {mw['p_value']:.4f}   "
          f"{'✓' if mw['p_value'] < 0.05 else '✗'}")
    print(f"  Precision@50:     {p50['precision_pcite']:.3f} vs "
          f"{p50['precision_traditional']:.3f}  ({p50['lift']:.1f}× lift)")
    print(f"  NDCG@50:          {ng['ndcg_pcite']:.4f} vs "
          f"{ng['ndcg_traditional']:.4f}")
    print(f"{'━'*55}\n")

    holds = (
        mw["p_value"] < 0.05
        and p50["lift"] > 1.0
        and ng["ndcg_pcite"] > ng["ndcg_traditional"]
    )
    print("✓ Hypothesis holds.\n" if holds else "✗ Hypothesis did not hold.\n")
    return 0 if holds else 1


if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--dry-run", action="store_true")
    sys.exit(asyncio.run(main(p.parse_args().dry_run)))
```

---

## Tests

Read these before reading any module. They run without API keys and define the behaviour
of every function in the system.

```python
"""tests/test_all.py"""

import pytest
from pcite.models import (
    Claim, Entity, PCite, PCiteType, Paper, Predicate, ProvenanceEntry,
    StatisticalQualifiers, ValidationClass, VALIDATION_WEIGHT, PCITE_WEIGHT,
)


def claim(doi: str = "10.1000/test") -> Claim:
    return Claim(
        subject    = Entity(id="HMDB:HMDB0000122", name="Glutamine", type="compound"),
        predicate  = Predicate.INCREASES,
        object     = Entity(id="MESH:D015212", name="Colitis, Ulcerative", type="disease"),
        qualifiers = StatisticalQualifiers(n=150, p_value=0.001, method="LC-MS"),
        provenance = [ProvenanceEntry(doi=doi, validation_class=ValidationClass.CURATED)],
    )


# Axiom 1: Claims are universal

def test_claim_id_excludes_doi():
    assert claim("10.1000/a").id == claim("10.1000/b").id

def test_merge_accumulates_provenance():
    assert len(claim("10.1000/a").merge(claim("10.1000/b")).provenance) == 2

def test_merge_idempotent_on_same_doi():
    c = claim("10.1000/a")
    assert len(c.merge(c).provenance) == 1

def test_merge_keeps_best_validation_class():
    c_phys = Claim(
        subject=claim().subject, predicate=claim().predicate,
        object=claim().object,   qualifiers=claim().qualifiers,
        provenance=[ProvenanceEntry(doi="10.1000/b",
                                    validation_class=ValidationClass.PHYSICAL,
                                    metabo_id="MTBLS123")],
    )
    assert claim("10.1000/a").merge(c_phys).validation_class == ValidationClass.PHYSICAL


# Axiom 2: Replication is structural

def test_replication_count_is_provenance_length():
    c = claim().merge(claim("10.1000/b")).merge(claim("10.1000/c"))
    assert c.replication_count == 3

def test_base_weight_increases_with_replication():
    assert claim().merge(claim("10.1000/b")).base_weight > claim().base_weight

def test_log_scaling_diminishes():
    """8th replication adds less than 2nd."""
    c = claim()
    for i in range(7):
        c = c.merge(claim(f"10.1000/{i}"))
    d2 = claim().merge(claim("10.1000/x")).base_weight - claim().base_weight
    d8 = c.merge(claim("10.1000/y")).base_weight - c.base_weight
    assert d2 > d8


# Axiom 3: CI pipeline is a validator

def test_invalid_p_value_raises():
    with pytest.raises(Exception): StatisticalQualifiers(p_value=1.5)

def test_invalid_ci_raises():
    with pytest.raises(Exception): StatisticalQualifiers(confidence_interval=(0.8, 0.3))

def test_zero_n_raises():
    with pytest.raises(Exception): StatisticalQualifiers(n=0)

def test_valid_qualifiers_pass():
    assert StatisticalQualifiers(n=100, p_value=0.05).n == 100


# Weight constants

def test_physical_is_highest():
    assert VALIDATION_WEIGHT[ValidationClass.PHYSICAL] == max(VALIDATION_WEIGHT.values())

def test_hypothesis_is_zero():
    assert VALIDATION_WEIGHT[ValidationClass.HYPOTHESIS] == 0.0

def test_replication_pcite_is_highest():
    assert PCITE_WEIGHT[PCiteType.REPLICATES] == max(PCITE_WEIGHT.values())

def test_edge_weight_formula():
    n = PCite(source_id="a", target_id="b", type=PCiteType.REPLICATES, source_weight=5.0)
    assert n.weight == pytest.approx(7.5)


# Validate: pure function tests (no network)

def test_metabo_deposit_upgrades_to_physical():
    from pcite.validate import classify_provenance
    paper = Paper(doi="10.1000/x", title="T", abstract="A", metabo_id="MTBLS123")
    entry = ProvenanceEntry(doi="10.1000/x")
    assert classify_provenance(entry, {"10.1000/x": paper}).validation_class \
           == ValidationClass.PHYSICAL

def test_abstract_upgrades_to_curated():
    from pcite.validate import classify_provenance
    paper = Paper(doi="10.1000/x", title="T", abstract="A")
    entry = ProvenanceEntry(doi="10.1000/x")
    assert classify_provenance(entry, {"10.1000/x": paper}).validation_class \
           == ValidationClass.CURATED

def test_three_curated_become_replicated():
    from pcite.validate import upgrade_claim
    c      = claim("10.1000/a").merge(claim("10.1000/b")).merge(claim("10.1000/c"))
    papers = {p.doi: Paper(doi=p.doi, title="T", abstract="A") for p in c.provenance}
    assert upgrade_claim(c, papers).validation_class == ValidationClass.REPLICATED

def test_two_curated_stay_curated():
    from pcite.validate import upgrade_claim
    c      = claim("10.1000/a").merge(claim("10.1000/b"))
    papers = {p.doi: Paper(doi=p.doi, title="T", abstract="A") for p in c.provenance}
    assert upgrade_claim(c, papers).validation_class == ValidationClass.CURATED


# Graph: scoring

def test_isolated_node_scores_zero():
    import networkx as nx
    from pcite.graph import compute_pcite_scores
    G = nx.DiGraph()
    G.add_node("a")
    assert compute_pcite_scores(G)["a"] == 0.0

def test_score_sums_incoming_weights():
    import networkx as nx
    from pcite.graph import compute_pcite_scores
    G = nx.DiGraph()
    for n in ["a","b","c"]: G.add_node(n)
    G.add_edge("b", "a", weight=2.0)
    G.add_edge("c", "a", weight=3.0)
    assert compute_pcite_scores(G)["a"] == pytest.approx(5.0)


# Evaluate: metrics

def test_ndcg_perfect_ranking():
    from pcite.evaluate import ndcg_at_k
    records = [{"claim_id": str(i), "validation_class": "PhysicalMeasurement",
                "pcite_score": float(10-i)} for i in range(10)]
    assert ndcg_at_k(records, k=10)["ndcg_pcite"] == pytest.approx(1.0)

def test_precision_in_range():
    from pcite.evaluate import precision_at_k
    records = [{"claim_id": str(i),
                "validation_class": "PhysicalMeasurement" if i < 10 else "AIGenerated",
                "pcite_score": float(i)} for i in range(100)]
    r = precision_at_k(records, k=10)
    assert 0.0 <= r["precision_pcite"] <= 1.0
```

---

## Phase 3 Thread

Every `ProvenanceEntry.metabo_id` stored through the pipeline is Phase 3.

Currently `metabo_id` classifies a claim as `PHYSICAL`. But it is also a direct link to
raw instrument files — `.mzML`, `.d`, `.raw` — sitting in MetaboLights. These are the
physical measurements that no AI can fabricate.

When Phase 3 begins, the extension is straightforward:

```python
class Measurement(BaseModel):
    instrument_id: str       # e.g. "Waters Xevo G2-XS"
    lab_ror:       str       # ROR identifier of the originating lab
    timestamp:     datetime
    sample_id:     str
    compound_id:   str       # HMDB ID
    intensity:     float
    raw_file_uri:  str       # direct MetaboLights link

    # links to → ProvenanceEntry.metabo_id → Claim
```

At that point the graph has a verifiable chain from a scientific claim to an actual
instrument reading. No AI-generated paper, regardless of how convincing, can produce
a mass spectrometer file linked to a verified deposit with a real instrument signature
and a real lab's ROR identifier.

That is the moat. And it is being built right now, invisibly, inside `metabo_id`.

---

## What Stays Proprietary

| Component | Reason |
|-----------|--------|
| Trained entity-linking models | Core IP |
| Institutional nImpact dashboard | Commercial product |
| Real-time extraction API | Monetised service |
| Instrument integration layer | Phase 3 moat |
| Fine-tuned citation classification model | Commercially valuable |

The open-source repository is the scientific contribution and the adoption mechanism.
The proprietary layer is what the company sells. They do not conflict.

---

## Definition of Done

`python run_poc.py` exits 0.

The four result lines print. All three tests agree. The top-10 claims by pCite score
are biologically sensible. A reviewer who clones the repo and runs `uv sync &&
python run_poc.py --dry-run` sees the same figures submitted with the paper.

That is the entire success criterion.
